[
  {
    "objectID": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html",
    "href": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html",
    "title": "Análisis de correlación en la Investigación en Psicología",
    "section": "",
    "text": "En esta sesión vamos a realizar las siguientes tareas:\n\nEntender la lógica de la correlación y su importancia\nReconocer el aporte de este análisis a la investigación en psicología\nConocer el procedimiento para ejecutar un análisis de la correlación en R"
  },
  {
    "objectID": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#objetivos",
    "href": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#objetivos",
    "title": "Análisis de correlación en la Investigación en Psicología",
    "section": "",
    "text": "En esta sesión vamos a realizar las siguientes tareas:\n\nEntender la lógica de la correlación y su importancia\nReconocer el aporte de este análisis a la investigación en psicología\nConocer el procedimiento para ejecutar un análisis de la correlación en R"
  },
  {
    "objectID": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#qué-es-la-correlación",
    "href": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#qué-es-la-correlación",
    "title": "Análisis de correlación en la Investigación en Psicología",
    "section": "¿Qué es la correlación?",
    "text": "¿Qué es la correlación?\nEs un análisis estadístico que nos permite evaluar la relación entre dos variables cuantitativas (al menos la mayor parte del tiempo).\nEs importante recordar que una correlación no involucra necesariamente una relación de causa y efecto.\nEl análisis de la correlación es una pieza importante en la estadística, en tanto puede observarse en el corazón de análisis más complejos como el modelado con ecuaciones estructurales."
  },
  {
    "objectID": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#coeficiente-de-correlación",
    "href": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#coeficiente-de-correlación",
    "title": "Análisis de correlación en la Investigación en Psicología",
    "section": "Coeficiente de correlación",
    "text": "Coeficiente de correlación\nIndicador que puede tomar valores entre -1 a +1 e informa sobre la fuerza y el sentido de la relación.\n\n\nTable 1: Interpretación del coeficiente de correlación\n\n\n\n\n\n\nSentido\nFuerza\n\n\n\n\nel signo positivo o negativo señala la dirección de la asociación.\n\nPositivo (directa): A mayor ansiedad hay mayor depresión (\\(\\rho_{xy} &gt; 0\\))\nNegativo (inversa): A mayor ansiedad hay menor bienestar (\\(\\rho_{xy} &lt; 0\\))\n\nel valor absoluto del coeficiente puede ser interpretado mediante el criterio de Cohen (1988).\n\n\\(\\rho_{xy} &gt; .1\\) = pequeño\n\\(\\rho_{xy} &gt; .3\\) = mediano\n\\(\\rho_{xy} &gt; .5\\) = grande"
  },
  {
    "objectID": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#correlación-de-pearson",
    "href": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#correlación-de-pearson",
    "title": "Análisis de correlación en la Investigación en Psicología",
    "section": "Correlación de Pearson",
    "text": "Correlación de Pearson\nEl coeficiente de asociación más común es el coeficiente de correlación de Pearson y su fórmula es la siguiente:\n\\[\n\\rho_{xy} = \\frac{\\Sigma(x_i - \\bar{x})(y_i - \\bar{y})} {\\sqrt{\\Sigma(x_i - \\bar{x})^2\\Sigma(y_i - \\bar{y})^2}}\n\\]\nDonde \\(r\\) es el coeficiente de correlación, \\(x_i\\) los valores de x en la muestra, \\(\\bar{x}\\) la media de los valores de x, \\(y_i\\) los valores de y en la muestra, \\(\\bar{y}\\) la media de los valores de y. De esta forma, se puede observar que el coeficiente de correlación de Pearson es un análisis paramétrico, en tanto hace uso de la media de los participantes para estimar la asociación entre las variables de interés. En caso no hubiese normalidad en los datos, se tendría que aplicar la correlación de Spearman."
  },
  {
    "objectID": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#normalidad",
    "href": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#normalidad",
    "title": "Análisis de correlación en la Investigación en Psicología",
    "section": "Normalidad",
    "text": "Normalidad\nDicho esto, es importante recalcar que, dado que la correlación requiere del uso de dos variables, su tipo de análisis se denomina “bivariado”. Así, para identificar el tipo de análisis a realizar (paramétrico o no paramétrico), es necesario evaluar la normalidad de la distribución de cada variable. La regla de decisión es la siguiente “si y solo si ambas variables tienen distribución normal, se aplicará la correlación de Pearson”.\nPara ilustrar esta situación, veamos un ejemplo. Imaginemos que contamos con la siguiente base de datos starwars .\n\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"The Empire Strikes Back\", \"Revenge of the Sith\", \"Return…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\nEn esta base, se desea evaluar la relación entre la masa y la altura de los personajes de la película. Para ello, primero necesitamos evaluar la normalidad de ambas variables.\n\nlibrary(rstatix)\n\nstarwars |&gt; \n  shapiro_test(height, mass)\n\n# A tibble: 2 × 3\n  variable statistic        p\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 height       0.878 1.45e- 6\n2 mass         0.235 6.26e-16\n\n\nComo indica la prueba Shapiro-Wilk, tanto la altura, \\(SW = .88, p &lt; .001\\), como la masa, \\(SW = .24, p &lt; .001\\), presentan una distribución no normal de los datos. Así, dado que no ambas no muestran normalidad, se habría de aplicar la prueba de correlación de Spearman."
  },
  {
    "objectID": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#patrones-de-correlación",
    "href": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#patrones-de-correlación",
    "title": "Análisis de correlación en la Investigación en Psicología",
    "section": "Patrones de correlación",
    "text": "Patrones de correlación\nHay muchas maneras de concebir la relación entre dos variables. Mayormente estos tipos de correlación toman su nombre a partir de la forma que buscan encontrar en los datos.\n\n\n\n\n\nFigure 1: Diferentes tipos de relación entre variables\n\n\n\n\nCabe mencionar que en este curso, nos centraremos en las relaciones de corte lineal. Este tipo de asociaciones son comunes en psicología dado que nos permiten una interpretación simple de la dinámica entre dos variables (p.ej. “A mas salud, mayor bienestar”)."
  },
  {
    "objectID": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#diagrama-de-dispersión",
    "href": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#diagrama-de-dispersión",
    "title": "Análisis de correlación en la Investigación en Psicología",
    "section": "Diagrama de dispersión",
    "text": "Diagrama de dispersión\nEl gráfico por excelente de la correlación es el gráfico de dispersión de puntos. Este gráfico se puede entender de la siguiente forma “a tal nivel de X, podemos encontrar esta Y”. A fin de entender con mayor facilidad este concepto, te pedimos que pases tu puntero por encima de los puntos del gráfico de dispersión, ¿podrías indicar el valor de x e y en cada uno?\n\n\n\n\n\n\nAhora que entiendes la lógica del gráfico de dispersión, es importante tener en cuenta la línea que se encuentra en medio. Esta línea representa la posible asociación lineal entre las variables de interés. Mas aún, pasa por lo que seria el medio de la nube de puntos, por lo que nos permite tener una descripción de la tendencia de ambas variables."
  },
  {
    "objectID": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#actividad-1-interpretando-el-diagrama-de-dispersión",
    "href": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#actividad-1-interpretando-el-diagrama-de-dispersión",
    "title": "Análisis de correlación en la Investigación en Psicología",
    "section": "Actividad 1: Interpretando el diagrama de dispersión",
    "text": "Actividad 1: Interpretando el diagrama de dispersión\nComo se mencionó anteriormente, el coeficiente de correlación puede tomar valores entre -1 y 1. Así, te pedimos que uses el siguiente widget y respondas a las siguientes preguntas:\n\n¿Que pasará si introduces valores entre 0 y 1 como coeficientes de correlación? ¿Cómo será la pendiente que forma la línea?\n¿Que pasara si introduces valores entre -1 y 0 como coeficientes de correlación? ¿Cómo será la pendiente que forma la línea?\n¿y si colocas el cero? ¿es una pendiente? ¿Por qué?\nColoca los valores de 1 y -1, ¿que similitudes y diferencias hay entre estos dos tipos de correlación?\n\n#| standalone: true\n#| viewerHeight: 500\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\nset.seed(123)\nrn &lt;- sample(seq(-1, 1, by = .1), 1)\n\n# Define UI for app that draws a histogram ----\nui &lt;- fluidPage(\n  numericInput(\"rc\", \"Coeficiente de correlación:\", \n  value = rn, min = -1, max = +1, step = .1),\n  plotOutput(\"dispPlot\"))\n\nserver &lt;- function(input, output, session) {\n\n  grf &lt;- function(rr) {\n  if (abs(rr) != 1) {\n    x1 &lt;- rnorm(100, 15, 5)\n    x2 &lt;- scale(matrix(rnorm(100), ncol= 1))\n    x12 &lt;- cbind(scale(x1),x2)\n    c1 &lt;- var(x12)\n    chol1 &lt;- solve(chol(c1))\n    newx &lt;-  x12 %*% chol1\n    zapsmall(cor(newx))\n    all.equal( x12[,1], newx[,1] )\n    newc &lt;- matrix( \n      c(1, rr,\n        rr, 1), ncol = 2)\n    eigen(newc)\n    chol2 &lt;- chol(newc)\n    finalx &lt;- newx %*% chol2 * sd(x1) + mean(x1)}\n  \n  if (rr == -1) {\n    x1 &lt;- rnorm(100, 15, 5)\n    x2 &lt;- -x1\n    finalx &lt;- data.frame(x1, x2)\n  }\n  \n  if (rr == 1) {\n    x1 &lt;- rnorm(100, 15, 5)\n    x2 &lt;- x1\n    finalx &lt;- data.frame(x1, x2)\n  }\n  \n  finalx |&gt; \n    data.frame() |&gt; \n    as_tibble() |&gt; \n    set_names(c(\"x\", \"y\"))\n}\n\noutput$dispPlot &lt;- renderPlot({\n  grf(input$rc) -&gt; cor_data\n  cor_data |&gt; \n    ggplot(aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = \"y ~ x\") +\n    theme_minimal() +\n    scale_x_continuous(limits = c(5, 25),\n                       breaks = seq(5, 25, by = 5)) +\n    labs(title = NULL,\n         x = \"Variable X\", y = \"Variable Y\") -&gt; p\n  \n  input$rc == -1 -&gt; sol\n  \n  if(sol) {\n    p + scale_y_continuous(limits = c(-25, -5),\n                           breaks = seq(-5, -25, by = -5))\n  } else {\n    p + scale_y_continuous(limits = c(5, 25),\n                           breaks = seq(5, 25, by = 5))\n  }\n})\n}\n# Create Shiny app ----\nshinyApp(ui = ui, server = server)\nComo puedes observar, la “pendiente” o caída de la recta va a ser diferente dependiendo del coeficiente de correlación que se tenga. En la Figure 2 podrás ver una figura resumen sobre las correlaciones y las tendencias que puede tener la línea.\n\n\n\n\n\nFigure 2: Diferentes valores de correlación\n\n\n\n\nDe este gráfico podemos extraer varias ideas:\n\nel sentido de la pendiente indica si la relación es positiva o negativa. Si la pendiente es descendente ( \\ ), estamos frente a una asociación negativa, caso contrario ( / ), la asociación sera directa o positiva.\nEn las asociaciones con valor absoluto de 1 vemos que la línea pasa por absolutamente todos los puntos. Además, conforme la relación se reduzca en valor absoluto, la nube de puntos empieza a dispersarse mas de la línea central.\nTradicionalmente, se entiende que las asociaciones de 1 y -1 son perfectas, de modo que X como Y son las mismas variables.\nEn la asociación con \\(r = 0\\) no se observa una tendencia de la nube de puntos. Aún más, la linea azul pasa a ser de una pendiente a un llano."
  },
  {
    "objectID": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#actividad-2-adivina-la-correlación",
    "href": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#actividad-2-adivina-la-correlación",
    "title": "Análisis de correlación en la Investigación en Psicología",
    "section": "Actividad 2: Adivina la correlación",
    "text": "Actividad 2: Adivina la correlación\nEn base a lo revisado en el bloque anterior, estas listo para intentar identificar el coeficiente de correlación de los siguientes gráficos. Para ello, observa bien el diagrama de dispersión de puntos y propón un valor de coeficiente en el espacio demarcado. Te recomendamos que pienses en términos de “positivo y negativo” y de magnitud del coeficiente (pequeño, mediano o grande) para estimar un posible valor. Puedes intentar este ejercicio cuantas veces lo requieras.\nTen en cuenta que esta mirada al gráfico es muy común en el análisis de datos, puesto que nos da una primera mirada sobre la posible asociación entre nuestras variables. Los gráficos de dispersión permiten a los grupos de investigación tener un primer vistazo de las asociaciones encontradas y pueden orientar a futuros análisis.\nFinalmente, en el siguiente apartado, aprenderás a estimar formalmente un coeficiente de correlación a partir de un conjunto de datos."
  },
  {
    "objectID": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#pasos-para-estimar-una-correlación",
    "href": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#pasos-para-estimar-una-correlación",
    "title": "Análisis de correlación en la Investigación en Psicología",
    "section": "Pasos para estimar una correlación",
    "text": "Pasos para estimar una correlación\n\nPlantear las hipótesis de trabajo (HT)\nPlantear las hipótesis estadísticas para cada HT\nExplorar la asociación con un diagrama de dispersión de puntos\nEvaluar la normalidad\nRealizar el análisis de correlación\nBrindar una respuesta o conclusión final"
  },
  {
    "objectID": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#ejemplo",
    "href": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#ejemplo",
    "title": "Análisis de correlación en la Investigación en Psicología",
    "section": "Ejemplo",
    "text": "Ejemplo\nUn grupo de estudiantes de la Facultad de Psicología, interesados en el trabajo de las Brigadas PUCP, planteó como objetivo general de su estudio, evaluar la relación que existe entre la resiliencia y la sintomatología de estrés post-traumático en un grupo de personas damnificadas por el terrorismo. Se espera que entre ambas variables se establezca una relación significativa. Dicho grupo te solicita que realices el análisis estadístico que pueda dar respuesta a su objetivo y que brindes tu conclusión en función a la hipótesis de trabajo.\nPaso 0: cargar los datos\nSi deseamos, podemos tener una visualización más elegante de la información. Para ello, abreviaremos a dos decimales todas las variables cuantitativas de la base y pediremos al paquete DT que nos genere una bonita tabla:\nPaso 1: hipótesis de trabajo\nHT: Existe relación entre los niveles de resiliencia (R) y la sintomatología de estrés post-traumático (S) de un grupo de personas damnificadas por el terrorismo.\nPaso 2: hipótesis estadísticas\n\\[\n\\displaylines{H_0:\\rho_{RS} = 0 \\\\ H_1:\\rho_{RS} \\neq 0}\n\\]\nPaso 3: diagrama de dispersión\n\nTraemos la base donde se encuentran las variables de interés\nDefinimos que variables tendremos en el eje “x” y en el eje “y”.\nGeneramos la dispersión de puntos\nColocamos la recta que nos puede ayudar a observar la tendencia y posible intensidad\nSeñalamos que queremos un tema minimalista (paso opcional).\n\nA partir del diagrama de dispersión, se observa que puede existir una relación negativa y mediana entre la resiliencia y en el estrés post traumático.\nPaso 4: evaluar la normalidad\nA partir de la prueba de normalidad de Shapiro-Wilk, se observa que el PTSD, \\(SW = .99, p = .59\\) y la resiliencia \\(SW = .98, p = .253\\), presentan una distribución normal de los datos. Por todo ello, se aplicará la correlación de Pearson.\nPaso 5: análisis de la correlación\n\nTraemos los datos\nusamos la función cor_test para generar la correlación\nEl método empleado fue el de Pearson, dada la normalidad\n\nConclusión: A partir del análisis de correlación de Pearson, se encontró una relación estadísticamente significativa, grande e inversa entre la resiliencia y el PTSD, \\(r_{(98)} = -.54, p &lt; .001\\).\n\n\n\n\n\n\nTip\n\n\n\nPara construir la conclusión en el formato APA, tenemos que fijarnos en los siguientes 4 elementos:\n\nSignificatividad: Por medio de valor p, identificamos si el resultado es significativo (p &lt; .05). En este caso, p es muy pequeño, incluso &lt; .001, por lo que es significativo.\nSentido: El signo de la correlación (cor), nos indica si la relación es negativa o positiva. En este caso, el signo negativo nos indica que la asociación es inversa.\nFuerza: El valor absoluto de la correlación (cor) nos permite identificar, según el criterio de Cohen (1988), la magnitud de la asociación. Ver Table 1. En este caso, la correlación es grande (r &gt; .5).\nGrados de libertad: N – 2, donde N es la cantidad de participantes. En este caso, 100 - 2 = 98."
  },
  {
    "objectID": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#ejercicios",
    "href": "1. Estadística Bivariada/1.2 Correlación/Análisis_de_la_Correlacion.html#ejercicios",
    "title": "Análisis de correlación en la Investigación en Psicología",
    "section": "Ejercicios",
    "text": "Ejercicios\nUn grupo de estudiantes de la Facultad de Psicología, interesados en la problemática de salud mental en jóvenes ha planteado una investigación con el objetivo de evaluar las relaciones entre salud percibida, bienestar social, resiliencia, depresión y estrés en un grupo de estudiantes universitarios. Ellos solicitan tu apoyo para realizar el análisis de sus datos. Resuelve las siguientes preguntas, teniendo en consideración los pasos señalados en clase.\n1. Plantea las hipótesis de trabajo (HT) y las hipótesis estadísticas para cada HT.\n2. Explora el patrón de relación entre las variables con un diagrama de dispersión y brinda una interpretación del gráfico.\n3. Evalúa la normalidad de los datos e indica el coeficiente de correlación pertinente a utilizar.\n4. Analiza estadísticamente las hipótesis estadísticas (H0 y H1). Aplica la regla de decisión.\n5. Brindar respuesta a las hipótesis de trabajo\nUtiliza el archivo de SPSS “Base – Ejercicios correlación”\n\nEl grupo de investigadores quiere comprobar si existe una relación entre la salud percibida y el bienestar social. ¿Se puede comprobar dicha asociación? Brinda tu conclusión.\n\n\nA partir de la revisión de la literatura, lxs investigadores esperan econtrar que la resiliencia se asocie negativamente con el estrés y la depresión. ¿Se comprueban las hipótesis? ¿La relación más fuerte se establece entre resiliencia y estrés? Brinda tu conclusión.\n\n\nPor último, el grupo se plantea explorar si existe una relación entre la salud percibida y la depresión  ¿Qué se puede concluir al respecto?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html",
    "href": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html",
    "title": "Análisis de Regresión Lineal Simple (RLS)",
    "section": "",
    "text": "Definir la regresión lineal y sus tipos\nEntender el concepto de variables independientes y dependientes\nRealizar un análisis de regresión lineal simple"
  },
  {
    "objectID": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#objetivos",
    "href": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#objetivos",
    "title": "Análisis de Regresión Lineal Simple (RLS)",
    "section": "",
    "text": "Definir la regresión lineal y sus tipos\nEntender el concepto de variables independientes y dependientes\nRealizar un análisis de regresión lineal simple"
  },
  {
    "objectID": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#definición",
    "href": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#definición",
    "title": "Análisis de Regresión Lineal Simple (RLS)",
    "section": "Definición",
    "text": "Definición\nAnálisis en el que una variable \\(Y\\) (de salida, respuesta o dependiente) se encuentra en relación de dependencia respecto a una o más variables predictoras \\(X_1, X_2, … X_p\\) (explicativas o independientes). Es importante tener en cuenta que variaciones o cambios en la Y derivan de las variaciones en otras variables \\(X_1, X_2,... X_p\\)"
  },
  {
    "objectID": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#clasificación",
    "href": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#clasificación",
    "title": "Análisis de Regresión Lineal Simple (RLS)",
    "section": "Clasificación",
    "text": "Clasificación\n\nSegún la cantidad de variables:\n\nSimple: Solo 1 variable predictora o independiente \\(X\\)\nMúltiple: Más de una variable predictora \\(X_1, X_2, … X_p\\)\n\nSegún el tipo de relación:\n\nLineales\nNo lineales\nNo lineales en Y (logísticas)"
  },
  {
    "objectID": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#variables-independientes-y-dependientes",
    "href": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#variables-independientes-y-dependientes",
    "title": "Análisis de Regresión Lineal Simple (RLS)",
    "section": "Variables independientes y dependientes",
    "text": "Variables independientes y dependientes\nExisten varias similitudes entre la correlación y la regresión lineal simple, en tanto ambas buscan establecer relaciones lineales entre dos variables. Sin embargo, hay importantes diferencias que se deben de tener en cuenta:\n\nA diferencia de la correlación, que solo establece una asociación entre dos variables, la regresión propone la noción de predicción. Esta es la diferencia mas importante, dado que los resultados de una variable \\(X\\) (independiente) se usan para predecir los resultados de una variable \\(Y\\) (dependiente).\nAsí, la regresión establece una direccionalidad. En el lenguaje de la correlación, entendíamos como equivalente decir “a mas ansiedad hay mas depresión” o “a mas depresión hay mas ansiedad”. Sin embargo, en la regresión, dado que las variables se ordenan como independientes o dependiente, solo podemos establecer una sola vía de interpretación. Por ejemplo, si decimos que la ansiedad predice menores niveles de bienestar, NO es posible indicar que menores niveles de bienestar predicen mayor ansiedad.\n\nEs común que al estudiar la relación entre dos variables, se piense en la noción de causa y efecto. La variable considerada causa, se le denomina “variable independiente” y la considerada efecto se denomina “variable dependiente”.\nSin embargo, en la investigación no experimental no es posible afirmar una relación causal, dado que el procedimiento de recojo de información no facilita una inferencia causal sobre las variables. Piensen en lo siguiente, si encuesto a las personas sobre ansiedad y bienestar a la vez, ¿puedo decir que la ansiedad va a impactar al bienestar?\nSin embargo, el análisis de la regresión lineal nos permite establecer relaciones de predicción, donde podemos evaluar si las variaciones en una variable independiente \\(X\\) predicen las variaciones que observaremos en una variable dependiente \\(Y\\)"
  },
  {
    "objectID": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#recta-de-la-regresión",
    "href": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#recta-de-la-regresión",
    "title": "Análisis de Regresión Lineal Simple (RLS)",
    "section": "Recta de la regresión",
    "text": "Recta de la regresión\nUn modelo de predicción, ecuación matemática que establece la relación de dependiente, puede visualizarse como una recta en un gráfico, en el que el eje horizontal (X) y el eje vertical representan los valores predichos de la variable dependiente (Y).\nLa recta a la que nos referimos se llama recta de regresión y lo hemos visto en los gráficos de dispersión de la clase de correlación."
  },
  {
    "objectID": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#ecuación-de-la-regresión-lineal-simple",
    "href": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#ecuación-de-la-regresión-lineal-simple",
    "title": "Análisis de Regresión Lineal Simple",
    "section": "Ecuación de la regresión lineal simple",
    "text": "Ecuación de la regresión lineal simple\n\\[\nY = \\alpha + \\beta_1X_1 \\pm \\epsilon\n\\]\nEn donde:\n\n\\(Y\\) es la variable a predecir\n\\(X_1\\) la variable predictora\n\\(\\alpha\\) (llamado también \\(\\beta_0\\)) y \\(\\beta_1\\) son parámetros desconocidos a estimar\n\\(\\epsilon\\) es el error que se comete en la predicción de los parámetros (error del modelo)"
  },
  {
    "objectID": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#actividad-1-que-significan-los-parámetros",
    "href": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#actividad-1-que-significan-los-parámetros",
    "title": "Análisis de Regresión Lineal Simple (RLS)",
    "section": "Actividad 1: ¿que significan los parámetros?",
    "text": "Actividad 1: ¿que significan los parámetros?\nEn la clase anterior, la recta del gráfico era estimada automáticamente para su visualización. Sin embargo, a fin identificar la función de cada uno de los parámetros de la regresión lineal simple vamos a ver un gráfico donde podrás cambiar libremente valores de \\(\\alpha\\) y \\(\\beta_1\\). Así, te pedimos que puedas contestar las siguientes preguntas:\n\n¿Que sucede cuando modificamos el valor del \\(\\alpha\\) sin mover el valor del \\(\\beta_1\\)?\n¿Qué sucede cuando modificamos el valor del \\(\\beta_1\\) sin mover el valor del \\(\\alpha\\)?\n¿Crees que puedas llegar a situar la recta en la mitad de la nube de puntos para indicarnos una posible tendencia de la predicción?\n\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(tidyverse)\n#library(latex2expr)\n\n# Define UI for app that draws a histogram ----\nui &lt;- fluidPage(\n  withMathJax(),\n  numericInput(\"intercept\", \"$\\alpha$: \", \n               value = 5, min = 0, max = 25, step = 1),\n  numericInput(\"slope\", \"$\\beta_1$:\", \n               value = 0, min = -1, max = +1, step = .1),\n  plotOutput(\"parPlot\")\n)\n\nserver &lt;- function(input, output, session) {\n  grf &lt;- function(rr) {\n    x1 &lt;- rnorm(100, 15, 5)\n    x2 &lt;- scale(matrix(rnorm(100), ncol= 1))\n    x12 &lt;- cbind(scale(x1),x2)\n    c1 &lt;- var(x12)\n    chol1 &lt;- solve(chol(c1))\n    newx &lt;-  x12 %*% chol1\n    zapsmall(cor(newx))\n    all.equal( x12[,1], newx[,1] )\n    newc &lt;- matrix( \n      c(1, rr,\n        rr, 1), ncol = 2)\n    eigen(newc)\n    chol2 &lt;- chol(newc)\n    finalx &lt;- newx %*% chol2 * sd(x1) + mean(x1)\n  finalx |&gt; \n    data.frame() |&gt; \n    as_tibble() |&gt; \n    set_names(c(\"x\", \"y\"))\n  }\n  \n  grf(.6) -&gt; crrr\n\nv &lt;- reactiveValues(intercept = NULL, slope = NULL)\n\nobserveEvent(input$intercept, {\n  v$intercept &lt;- input$intercept\n})\n\nobserveEvent(input$slope, {\n  v$slope &lt;- input$slope\n})\n\noutput$parPlot &lt;- renderPlot({\n  crrr |&gt; \n    ggplot(aes(x = x, y = y)) +\n    geom_point() +\n    # geom_smooth(method = \"lm\", formula = \"y ~ x\") +\n    geom_abline(slope = v$slope, intercept = v$intercept, aes(colour = \"blue\")) +\n    theme_minimal() +\n    labs(title = NULL,\n         x = \"Variable X\", y = \"Variable Y\") +\n    scale_x_continuous(limits = c(5, 25),\n                             breaks = seq(5, 25, by = 5)) +\n    scale_y_continuous(limits = c(5, 25),\n                             breaks = seq(5, 25, by = 5))\n})\n}\n# Create Shiny app ----\nshinyApp(ui = ui, server = server)\nEn efecto, como puedes ver los parámetros aluden a la pendiente de la linea (\\(\\beta_1\\)) y su posición cuando cruza el eje Y (\\(\\alpha\\)). Más aún, si bien podemos aproximarnos, hasta cierto punto, a colocar la línea en la mitad de la nube de puntos, no tenemos una precisión exacta de esta posición.\nPor todo ello, necesitamos de un estimador que nos permita ubicar de manera correcta a la línea en medio de los puntos. En el caso de la regresión, esa estimación tiene el nombre de “mínimos cuadrados” (least squares). Si bien en el curso, no vamos a profundizar en este estimador, es importante que denotemos la necesidad de usar este tipo de funciones matemáticas para poder ubicar los parámetros idóneos de un modelo."
  },
  {
    "objectID": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#ecuación-de-la-rls",
    "href": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#ecuación-de-la-rls",
    "title": "Análisis de Regresión Lineal Simple (RLS)",
    "section": "Ecuación de la RLS",
    "text": "Ecuación de la RLS\n\\[\nY = \\alpha + \\beta_1X_1 \\pm \\epsilon\n\\]\nEn donde:\n\n\\(Y\\) es la variable a predecir\n\\(X_1\\) la variable predictora\n\\(\\alpha\\) (llamado también \\(\\beta_0\\)) y \\(\\beta_1\\) son parámetros desconocidos a estimar\n\\(\\epsilon\\) es el error que se comete en la predicción de los parámetros (error del modelo)"
  },
  {
    "objectID": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#contraste-de-hipótesis-para-la-rls",
    "href": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#contraste-de-hipótesis-para-la-rls",
    "title": "Análisis de Regresión Lineal Simple (RLS)",
    "section": "Contraste de hipótesis para la RLS",
    "text": "Contraste de hipótesis para la RLS\n\n\n\n\n\n\n\n\nHipótesis\nNotación\nInterpretación\n\n\n\n\nNula\n\\(H_0: R^2 = 0\\)\nNo hay relación predictiva\n\n\nBilateral\n\\(H_1: R^2 \\neq 0\\)\nX predice Y\n\n\nUnilateral derecho\n\\(H_1: R^2 &gt; 0\\)\nX predice positivamente Y\n\n\nUnilateral izquierdo\n\\(H_1: R^2 &lt; 0\\)\nX predice negativamente Y"
  },
  {
    "objectID": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#supuestos",
    "href": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#supuestos",
    "title": "Análisis de Regresión Lineal Simple (RLS)",
    "section": "Supuestos",
    "text": "Supuestos\nEs importante que, antes de la regresión, podamos establecer si existe alguna evidencia de asociación lineal entre las variables. Para ello, hemos de analizar la correlación de Pearson o Spearman (según la normalidad de los datos) que corresponda a las hipótesis estadísticas del problema.\n\n\n\nHipótesis de Correlación\nHipótesis de Regresión\n\n\n\n\n\\(H_0: r_{xy} = 0\\)\n\\(H_0: R^2 = 0\\)\n\n\n\\(H_0: r_{xy} \\neq 0\\)\n\\(H_1: R^2 \\neq 0\\)\n\n\n\\(H_1: r_{xy} &gt; 0\\)\n\\(H_1: R^2 &gt; 0\\)\n\n\n\\(H_1: r_{xy} &lt; 0\\)\n\\(H_1: R^2 &lt; 0\\)"
  },
  {
    "objectID": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#bondad-del-ajuste",
    "href": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#bondad-del-ajuste",
    "title": "Análisis de Regresión Lineal Simple (RLS)",
    "section": "Bondad del Ajuste",
    "text": "Bondad del Ajuste\n\nCoeficiente de determinación (\\(R^2\\)): permite evaluar la proporción de variabilidad de Y explicada por X. La magnitud de este coeficiente se puede interpretar a partir del criterio de Cohen (1988) de correlación elevado al cuadrado.\n\n\n\n\nIntevalo\nMagnitud\n\n\n\n\n0 a .01\nIrrelevante\n\n\n.01 a .09\nPequeño\n\n\n.09 a .25\nMediano\n\n\n.25 a 1\nAlto o grande\n\n\n\n\nAnálisis de varianza: permite valorar hasta qué punto es adecuado el modelo de regresión lineal para estimar los valores de la variable (Y). El contraste de hipótesis es el siguiente:\n\\[\n\\displaylines{H_0 = R^2 = 0\\\\ H_1 = R^2 \\neq 0}\n\\]"
  },
  {
    "objectID": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#estimación-de-parámetros",
    "href": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#estimación-de-parámetros",
    "title": "Análisis de Regresión Lineal Simple (RLS)",
    "section": "Estimación de parámetros",
    "text": "Estimación de parámetros\nLos parámetros de la regresión responden a los valores necesarios para completar la ecuación de la regresión. Es importante tener en cuenta que estos valores nos pueden permitir estimar un posible valor de Y cuando X cumple un determinado valor. Por ejemplo:\n\nImaginemos que tenemos que estimar la siguiente regresión donde la procrastinación (X) predice un menor nivel de rendimiento académico (Y).\n\\(Y = B_0 + B_1X_1 \\pm \\epsilon\\)\nAl realizar la regresión, podemos cambiar las incógnitas por sus respectivos valores:\n\\(Y = 2.14 + 1.4(X_1) \\pm .8\\)\nAhora imaginemos que tenemos un participante con 10 puntos en la escala de procastinación, ¿cuál sería su valor estimado de rendimiento académico?\n\n\\(Y = 2.14 + 1.4(10) \\pm .8 = [15.34, 16.94]\\)\n\nEl rendimiento esperado se encontraría dentro del intervalo de confianza de 15.34 a 16.94."
  },
  {
    "objectID": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#ejemplo-1",
    "href": "2. Estadística Multivariada/2.1 Regresión Lineal Simple/Regresión_Lineal_Simple.html#ejemplo-1",
    "title": "Análisis de Regresión Lineal Simple (RLS)",
    "section": "Ejemplo 1",
    "text": "Ejemplo 1\nUn grupo de investigadores plantea que el afrontamiento centrado en el problema predice un menor estrés académico percibido por los estudiantes. Para evaluar su hipótesis tomaron una muestra de 72 jóvenes quienes respondieron a medidas de afrontamiento y estrés académico.\n\ndata &lt;- haven::read_sav(\"https://github.com/renatoparedes/EstadisticaYPsicologiaMatematica/raw/main/INEE/Clase9_BaseRegresionLinealSimpleYMultiple.sav\")\n\ndata\n\n# A tibble: 72 × 8\n   Código Sexo       Relación HistVinc Afroproblema Afroemoción Estrés_académico\n   &lt;chr&gt;  &lt;dbl+lbl&gt;  &lt;dbl+lb&gt;    &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;\n 1 001    2 [Femeni… 1 [Si]          6         29.2          38               45\n 2 002    2 [Femeni… 0 [No]          7         23.8          45               45\n 3 003    2 [Femeni… 0 [No]          8         24.6          50               45\n 4 004    2 [Femeni… 0 [No]          5         21.5          45               45\n 5 005    2 [Femeni… 1 [Si]          8         30            49               44\n 6 006    2 [Femeni… 1 [Si]          6         33.1          48               44\n 7 007    1 [Mascul… 0 [No]          6         31.5          44               43\n 8 008    1 [Mascul… 1 [Si]         11         21.5          36               43\n 9 009    2 [Femeni… 0 [No]          8         22.3          35               43\n10 010    2 [Femeni… 1 [Si]          8         23.8          32               42\n# ℹ 62 more rows\n# ℹ 1 more variable: ZRE_1 &lt;dbl&gt;\n\n\n\nIndica las hipótesis de estudio, la ecuación del modelo de regresión lineal simple y el signo que debiera tener beta (\\(\\beta\\)) si fuera cierta la \\(H_1\\).\nEvalúa si habría evidencia a favor de una posible relación entre las variables con un diagrama de dispersión.\nEvalúa la correlación entre las variables.\nEstima los parámetros del modelo de regresión.\nEvalúa la bondad del ajuste del modelo\nRealiza el contraste de hipótesis correspondiente y concluir en función a la hipótesis de estudio.\n\n\nPaso 1. Planteamiento del ejercicio\nHT: El afrontamiento centrado en el problema (X) predice un menor estrés académico (Y) percibido por los estudiantes.\nVariable predictora (X) = Afrontamiento centrado en el problema (ACP)\nVariable respuesta (Y) = Estrés Acádémico (E)\n\\(E = \\alpha + \\beta_{ACP}X_{ACP} \\pm e, con\\ \\beta &lt; 0\\)\nHipótesis Estadísticas:\n\\[\n\\displaylines{H_0 : \\beta_{ACP} = 0 \\\\ H_1 : \\beta_{ACP} &lt; 0\\ (unilateral)}\n\\]\n\n\nPaso 2. Evidencias de posible relación\n\ndata %&gt;%\n  ggplot(aes(x = Afroproblema, y = Estrés_académico)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  labs(x = \"Afrontamiento centrado en la emoción\",\n       y = \"Estrés Académico\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nPaso 3. Correlación entre variables\n\\(\\displaylines{H_0: Hay\\ normalidad \\\\ H_1: No\\ hay\\ normalidad}\\)\n\ndata %&gt;%\n  shapiro_test(Afroproblema, Estrés_académico)\n\n# A tibble: 2 × 3\n  variable         statistic       p\n  &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt;\n1 Afroproblema         0.974 0.143  \n2 Estrés_académico     0.936 0.00119\n\n\nA partir de la prueba de Shapiro-Wilk, se observa que, si bien el afrontamiento centrado al problema muestra una distribución normal, \\(SW = .974, p = .143\\), el estrés académico no muestra una distribución normal de los datos, \\(SW = .936, p = .001\\). Por todo ello, se hará uso de un análisis no paramétrico.\n\\(\\displaylines{H_0: r_{ACPE} = 0 \\\\ H_1: r_{ACPE} &lt; 0\\ (unilateral)}\\)\n\ndata %&gt;%\n  cor_test(Afroproblema, Estrés_académico, method = \"spearman\")\n\n# A tibble: 1 × 6\n  var1         var2               cor statistic       p method  \n  &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   \n1 Afroproblema Estrés_académico -0.37    85007. 0.00153 Spearman\n\n\n\\(p_{unilateral} = .00153/2 = .00077\\)\nA partir de la correlación de Spearman, se observa que existe una relación estadísticamente significativa, negativa y mediana entre el afrontamiento centrado en el problema y el estrés académico, \\(r_{(70)} = -.37, p &lt; .001\\ (unilateral)\\).\n\n\nPaso 4. Estimar los parámetros de la regresión\n\nlm(Estrés_académico ~ Afroproblema, data = data) -&gt; m1\nsummary(m1)\n\n\nCall:\nlm(formula = Estrés_académico ~ Afroproblema, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.3968  -4.9886  -0.5203   4.5205  14.4468 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   46.9092     3.5161   13.34  &lt; 2e-16 ***\nAfroproblema  -0.5247     0.1399   -3.75  0.00036 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.443 on 70 degrees of freedom\nMultiple R-squared:  0.1673,    Adjusted R-squared:  0.1554 \nF-statistic: 14.07 on 1 and 70 DF,  p-value: 0.0003603\n\n\nLa ecuación del modelo sería la siguiente:\n\\(E = 46.9092 - .5247X_{ACP} \\pm 7.443\\)\n\n\nPaso 5. Bondad del ajuste\n\nsummary(m1)\n\n\nCall:\nlm(formula = Estrés_académico ~ Afroproblema, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.3968  -4.9886  -0.5203   4.5205  14.4468 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   46.9092     3.5161   13.34  &lt; 2e-16 ***\nAfroproblema  -0.5247     0.1399   -3.75  0.00036 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.443 on 70 degrees of freedom\nMultiple R-squared:  0.1673,    Adjusted R-squared:  0.1554 \nF-statistic: 14.07 on 1 and 70 DF,  p-value: 0.0003603\n\n\n\nCoeficiente de determinación: El coeficiente de determinación es mediano y explica el 16.73% de la variabilidad del estrés académico, \\(R^2 = .17\\).\nAnálisis de varianza:\n\\(\\displaylines{H_0: R^2 = 0 \\\\ H_1:R^2 \\neq 0}\\)\nEl modelo es adecuado para ser interpretado, \\(F(70) = 14.07, p &lt; .001\\).\n\n\n\nPaso 6. Contraste de Hipótesis\n\nsummary(m1)\n\n\nCall:\nlm(formula = Estrés_académico ~ Afroproblema, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.3968  -4.9886  -0.5203   4.5205  14.4468 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   46.9092     3.5161   13.34  &lt; 2e-16 ***\nAfroproblema  -0.5247     0.1399   -3.75  0.00036 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.443 on 70 degrees of freedom\nMultiple R-squared:  0.1673,    Adjusted R-squared:  0.1554 \nF-statistic: 14.07 on 1 and 70 DF,  p-value: 0.0003603\n\n\nLos elementos que tenemos que evaluar en el contraste son los siguientes:\n\n\n\n\n\n\n\nCriterio\nResultado\n\n\n\n\nSignificatividad (Contraste de hipótesis)\nDado que \\(p_{unilateral} = .00036/2 = .00018\\) se rechaza \\(H_0\\).\n\n\nSentido\nEl coeficiente Beta no estandarizado es -.53, por lo que el sentido es negativo\n\n\nGrados de Libertad\n\\(N-k-1\\), donde \\(N\\) es la cantidad total de participantes y \\(k\\) la cantidad de variables independientes (predictoras) en el modelo. De esta forma, tenemos \\(72-1-1=70\\) grado de libertad.\n\n\n\nA partir del análisis de regresión, se observa que el afrontamiento centrado en el problema es un predictor estadísticamente significativo y negativo del estrés académico, \\(\\beta_{ACP} = -.53, t_{(70)} = -3.75, p_{unilateral} &lt; .001\\).\n\n\nPaso 7. Conclusión final\nA partir del análisis de regresión lineal simple, con el afrontamiento centrado en el problema como predictor del estrés académico, se encontró que existe un modelo adecuado con un coeficiente de determinación mediano que explica el 16.73% de la variabilidad del estrés, \\(F(70) = 14.07, p &lt; .001, R^2 = .17\\). Específicamente, se encontró que el afrontamiento centrado en el problema es un predictor estadísticamente significativo y negativo del estrés académico, \\(\\beta_{ACP} = -.53, t_{(70)} = -3.75, p_{unilateral} &lt; .001\\). Por todo ello, se puede concluir que se cumple con la hipótesis del equipo de investigación."
  }
]